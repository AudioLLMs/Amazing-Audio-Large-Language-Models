# AudioLLMs

This repository is a curated collection of research papers focused on the development, implementation, and evaluation of language models for audio data. Our goal is to provide researchers and practitioners with a comprehensive resource to explore the latest advancements in AudioLLMs. Contributions and suggestions for new papers are highly encouraged!

## Papers on Models

|  Date   |       Model          |    Key Affiliations    | Paper |    Link     |
| :-----: | :------------------: | :--------------------: | :---- | :---------: |
| 2024-07 |     Qwen2-Audio      |      Alibaba           | Qwen2-Audio Technical Report | [Paper](https://arxiv.org/abs/2407.10759) / [Code](https://github.com/QwenLM/Qwen2-Audio) |
| 2024-07 |     FunAudioLLM      |      Alibaba               | FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs | [Paper](https://arxiv.org/pdf/2407.04051v3) / [Code](https://github.com/FunAudioLLM) / [Demo](https://fun-audio-llm.github.io/)  |
| 2024-05 |     SpeechVerse      |      AWS               | SpeechVerse: A Large-scale Generalizable Audio Language Model | [Paper](https://arxiv.org/pdf/2405.08295) |
| 2024-04 |     SALMONN          |      Tsinghua          | SALMONN: Towards Generic Hearing Abilities for Large Language Models | [Paper](https://arxiv.org/pdf/2310.13289.pdf) / [Code](https://github.com/bytedance/SALMONN) / [Demo](https://huggingface.co/spaces/tsinghua-ee/SALMONN-7B-gradio) |
| 2024-03 |     WavLLM           |      CUHK              | WavLLM: Towards Robust and Adaptive Speech Large Language Model | [Paper](https://arxiv.org/pdf/2404.00656) / [Code](https://github.com/microsoft/SpeechT5/tree/main/WavLLM) |
| 2024-01 |     Pengi            |      Microsoft         | Pengi: An Audio Language Model for Audio Tasks | [Paper](https://arxiv.org/pdf/2305.11834.pdf) / [Code](https://github.com/microsoft/Pengi) |
| 2023-12 |     Qwen-Audio       |      Alibaba           | Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models | [Paper](https://arxiv.org/pdf/2311.07919.pdf) / [Code](https://github.com/QwenLM/Qwen-Audio) / [Demo](https://qwen-audio.github.io/Qwen-Audio/) |
| 2023-12 |     LTU-AS           |      MIT               | Joint Audio and Speech Understanding | [Paper](https://arxiv.org/pdf/2309.14405v3.pdf) / [Code](https://github.com/YuanGongND/ltu) / [Demo](https://huggingface.co/spaces/yuangongfdu/ltu-2) |
| 2023-10 |     UniAudio         |      CUHK              | An Audio Foundation Model Toward Universal Audio Generation | [Paper](https://arxiv.org/abs/2310.00704) / [Code](https://github.com/yangdongchao/UniAudio) / [Demo](https://dongchaoyang.top/UniAudio_demo/) |
| 2023-09 |     LLaSM            |      LinkSoul.AI       | LLaSM: Large Language and Speech Model | [Paper](https://arxiv.org/pdf/2308.15930.pdf) / [Code](https://github.com/LinkSoul-AI/LLaSM) |
| 2023-06 |     AudioPaLM        |      Google            | AudioPaLM: A Large Language Model that Can Speak and Listen | [Paper](https://arxiv.org/pdf/2306.12925.pdf) / [Demo](https://google-research.github.io/seanet/audiopalm/examples/) |
| 2023-05 |     VioLA            |      Microsoft         | VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation | [Paper](https://arxiv.org/pdf/2305.16107.pdf) |
| 2023-05 |     SpeechGPT        |      Fudan             | SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities | [Paper](https://arxiv.org/pdf/2305.11000.pdf) / [Code](https://github.com/0nutation/SpeechGPT/tree/main/speechgpt) / [Demo](https://0nutation.github.io/SpeechGPT.github.io/) |
| 2023-04 |     AudioGPT         |      Zhejiang Uni      | AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head | [Paper](https://arxiv.org/pdf/2304.12995.pdf) / [Code](https://github.com/AIGC-Audio/AudioGPT) |
| 2022-09 |     AudioLM          |      Google            | AudioLM: a Language Modeling Approach to Audio Generation | [Paper](https://arxiv.org/abs/2209.03143) / [Demo](https://google-research.github.io/seanet/audiolm/examples/) |
